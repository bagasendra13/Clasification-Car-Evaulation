{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6M9f-msHbc8t"
      },
      "outputs": [],
      "source": [
        "# Import library yang dibutuhkan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "\n",
        "# Set pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Set random seed untuk reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdiQAFxqfV6x",
        "outputId": "013d7916-c338-499f-9095-5beffc691ad5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-24 19:22:39--  https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘car.data.1’\n",
            "\n",
            "car.data.1              [ <=>                ]  50.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-06-24 19:22:39 (525 KB/s) - ‘car.data.1’ saved [51867]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 1. LOAD DATA DAN EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ===============================================================\n",
        "print(\"1. Loading data and performing EDA...\")\n",
        "\n",
        "# Load dataset\n",
        "column_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Load dataset (adjust path as needed)\n",
        "df = pd.read_csv('car.data', names=column_names, skipinitialspace=True, na_values=\"?\")\n",
        "\n",
        "# Dataset Overview\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(df.info())\n",
        "\n",
        "# Descriptive Statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Categorical Statistics\n",
        "print(\"\\nCategorical Statistics:\")\n",
        "print(df.describe(include=['object']))\n",
        "\n",
        "# Check Missing Values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_values)\n",
        "print(\"\\nMissing Percentage:\")\n",
        "print(missing_percentage)\n",
        "\n",
        "# Target Distribution\n",
        "print(\"\\nTarget Distribution:\")\n",
        "print(df['class'].value_counts(normalize=True))\n",
        "\n",
        "# The original data visualization is moved to after encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSwORF0mfCLd",
        "outputId": "d59de447-51bf-49d0-f75d-da82efd634ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading data and performing EDA...\n",
            "Dataset Overview:\n",
            "Shape: (1728, 7)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1728 entries, 0 to 1727\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   buying    1728 non-null   object\n",
            " 1   maint     1728 non-null   object\n",
            " 2   doors     1728 non-null   object\n",
            " 3   persons   1728 non-null   object\n",
            " 4   lug_boot  1728 non-null   object\n",
            " 5   safety    1728 non-null   object\n",
            " 6   class     1728 non-null   object\n",
            "dtypes: object(7)\n",
            "memory usage: 94.6+ KB\n",
            "None\n",
            "\n",
            "Descriptive Statistics:\n",
            "       buying  maint doors persons lug_boot safety  class\n",
            "count    1728   1728  1728    1728     1728   1728   1728\n",
            "unique      4      4     4       3        3      3      4\n",
            "top     vhigh  vhigh     2       2    small    low  unacc\n",
            "freq      432    432   432     576      576    576   1210\n",
            "\n",
            "Categorical Statistics:\n",
            "       buying  maint doors persons lug_boot safety  class\n",
            "count    1728   1728  1728    1728     1728   1728   1728\n",
            "unique      4      4     4       3        3      3      4\n",
            "top     vhigh  vhigh     2       2    small    low  unacc\n",
            "freq      432    432   432     576      576    576   1210\n",
            "\n",
            "Missing Values:\n",
            "buying      0\n",
            "maint       0\n",
            "doors       0\n",
            "persons     0\n",
            "lug_boot    0\n",
            "safety      0\n",
            "class       0\n",
            "dtype: int64\n",
            "\n",
            "Missing Percentage:\n",
            "buying      0.0\n",
            "maint       0.0\n",
            "doors       0.0\n",
            "persons     0.0\n",
            "lug_boot    0.0\n",
            "safety      0.0\n",
            "class       0.0\n",
            "dtype: float64\n",
            "\n",
            "Target Distribution:\n",
            "class\n",
            "unacc    0.700231\n",
            "acc      0.222222\n",
            "good     0.039931\n",
            "vgood    0.037616\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 2. VISUALISASI DATA\n",
        "# ===============================================================\n",
        "print(\"\\n2. Data Visualization...\")\n",
        "\n",
        "# Bar plot for categorical variables (all features are treated as categorical after encoding)\n",
        "categorical_features = df.columns # All columns are categorical after encoding\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    df[feature].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel(feature)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'distribution_{feature}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Correlation matrix is now handled after encoding in step 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d8aIQm0fKjD",
        "outputId": "bde0294a-e3fe-4392-e40a-0cda4678ff4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Data Visualization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 3. DATA CLEANING\n",
        "# ===============================================================\n",
        "print(\"\\n3. Data Cleaning...\")\n",
        "\n",
        "# Handle missing values\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        # For categorical columns, fill with mode\n",
        "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "    else:\n",
        "        # For numeric columns, fill with median\n",
        "        df[column].fillna(df[column].median(), inplace=True)\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Number of rows after removing duplicates: {len(df)}\")"
      ],
      "metadata": {
        "id": "P14GRSkbe_g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e49992f-8b94-449c-8542-ac3766715893"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Data Cleaning...\n",
            "Number of duplicate rows: 0\n",
            "Number of rows after removing duplicates: 1728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-2488419309.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[column].fillna(df[column].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 4. FEATURE ENGINEERING & ENCODING\n",
        "# ===============================================================\n",
        "print(\"\\n4. Feature Engineering & Encoding...\")\n",
        "\n",
        "# Store encoding maps\n",
        "encoding_maps = {}\n",
        "\n",
        "# Encoding for 'buying'\n",
        "buying = {'vhigh': 4, 'high': 3, 'med': 2, 'low': 1}\n",
        "df['buying'] = df['buying'].map(buying)\n",
        "encoding_maps['buying'] = buying\n",
        "\n",
        "# Encoding for 'maint'\n",
        "maint = {'vhigh': 4, 'high': 3, 'med': 2, 'low': 1}\n",
        "df['maint'] = df['maint'].map(maint)\n",
        "encoding_maps['maint'] = maint\n",
        "\n",
        "# Encoding for 'doors'\n",
        "doors = {'2': 2, '3': 3, '4': 4, '5more': 5}\n",
        "df['doors'] = df['doors'].map(doors)\n",
        "encoding_maps['doors'] = doors\n",
        "\n",
        "# Encoding for 'persons'\n",
        "persons = {'2': 2, '4': 4, 'more': 5}\n",
        "df['persons'] = df['persons'].map(persons)\n",
        "encoding_maps['persons'] = persons\n",
        "\n",
        "# Encoding for 'lug_boot'\n",
        "lug_boot = {'small': 1, 'med': 2, 'big': 3}\n",
        "df['lug_boot'] = df['lug_boot'].map(lug_boot)\n",
        "encoding_maps['lug_boot'] = lug_boot\n",
        "\n",
        "# Encoding for 'safety'\n",
        "safety = {'low': 1, 'med': 2, 'high': 3}\n",
        "df['safety'] = df['safety'].map(safety)\n",
        "encoding_maps['safety'] = safety\n",
        "\n",
        "# Encoding for 'class' (target)\n",
        "class_ = {'unacc': 1, 'acc': 2, 'good': 3, 'vgood': 4}\n",
        "df['class'] = df['class'].map(class_)\n",
        "encoding_maps['class'] = class_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsV1dB88e96L",
        "outputId": "d75778a2-9b58-4272-efa9-8d89c06f9a98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. Feature Engineering & Encoding...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 5. CHECKING CORRELATIONS AFTER ENCODING\n",
        "# ===============================================================\n",
        "print(\"\\n5. Checking correlations after encoding...\")\n",
        "\n",
        "# Create correlation heatmap\n",
        "correlation = df.corr()\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(correlation.round(2),\n",
        "           annot=True,\n",
        "           vmax=1,\n",
        "           square=True,\n",
        "           cmap='RdYlGn_r')\n",
        "plt.title('Correlation Matrix After Encoding')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_after_encoding.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "b4ZlJSNge8uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087c884f-3534-4efa-e90d-1d72a6e65d3f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5. Checking correlations after encoding...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 6. FEATURE SELECTION\n",
        "# ===============================================================\n",
        "print(\"\\n6. Feature Selection...\")\n",
        "\n",
        "# Remove constant features\n",
        "df = df.loc[:, df.apply(pd.Series.nunique) != 1]\n",
        "\n",
        "# Find and remove highly correlated features\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "\n",
        "data_tanpa_fitur = df.drop('class', axis=1)\n",
        "corr_features = correlation(data_tanpa_fitur, 0.8)\n",
        "print('Correlated features: ', len(set(corr_features)))\n",
        "print(corr_features)\n",
        "\n",
        "# Remove highly correlated features\n",
        "df.drop(labels=corr_features, axis=1, inplace=True)\n",
        "\n",
        "# Final correlation check\n",
        "correlation = df.corr()\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(correlation.round(2),\n",
        "           annot=True,\n",
        "           vmax=1,\n",
        "           square=True,\n",
        "           cmap='RdYlGn_r')\n",
        "plt.title('Final Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig('final_correlation.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "HcTE4H-re7Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e739fbc-9456-4523-f359-36f16edbec54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6. Feature Selection...\n",
            "Correlated features:  0\n",
            "set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 7. MODEL TRAINING\n",
        "# ===============================================================\n",
        "print(\"\\n7. Model Training...\")\n",
        "\n",
        "# Split data into features and target\n",
        "X = df.drop('class', axis=1)\n",
        "y = df['class']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create decision tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt_classifier,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")"
      ],
      "metadata": {
        "id": "HS9XU7tEe5Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6279b164-d9fc-4f12-a03f-f88c1cf5034a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7. Model Training...\n",
            "Training set size: 1555 samples\n",
            "Testing set size: 173 samples\n",
            "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
            "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 8. MODEL EVALUATION\n",
        "# ===============================================================\n",
        "print(\"\\n8. Model Evaluation...\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=sorted(df['class'].unique()),\n",
        "           yticklabels=sorted(df['class'].unique()))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Visualization of the Decision Tree\n",
        "plt.figure(figsize=(40, 30))\n",
        "plot_tree(best_model,\n",
        "         feature_names=X.columns,\n",
        "         class_names=[str(i) for i in sorted(df['class'].unique())],\n",
        "         filled=True,\n",
        "         rounded=True,\n",
        "         max_depth=3)  # Limiting depth for better visualization\n",
        "plt.title('Decision Tree Visualization (Limited to Depth 3)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('decision_tree.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': best_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.close()\n",
        "\n",
        "# Check performance on training data\n",
        "ori_y_pred_dt_train = best_model.predict(X_train)\n",
        "\n",
        "ori_accuracy_dt_train = accuracy_score(y_train, ori_y_pred_dt_train)\n",
        "print('Akurasi pada training set: ', ori_accuracy_dt_train)\n",
        "\n",
        "ori_precision_dt_train = precision_score(y_train, ori_y_pred_dt_train, average='micro')\n",
        "print('Precision pada training set: ', ori_precision_dt_train)\n",
        "\n",
        "ori_recall_dt_train = recall_score(y_train, ori_y_pred_dt_train, average='micro')\n",
        "print('Recall pada training set: ', ori_recall_dt_train)\n",
        "\n",
        "# Recheck performance on testing data\n",
        "ori_accuracy_dt_test = accuracy_score(y_test, y_pred)\n",
        "print('Akurasi pada test set: ', ori_accuracy_dt_test)\n",
        "\n",
        "ori_precision_dt_test = precision_score(y_test, y_pred, average='micro')\n",
        "print('Precision pada test set: ', ori_precision_dt_test)\n",
        "\n",
        "ori_recall_dt_test = recall_score(y_test, y_pred, average='micro')\n",
        "print('Recall pada test set: ', ori_recall_dt_test)\n"
      ],
      "metadata": {
        "id": "okpVSXKpe161",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cc14b1-727b-4f45-d6d0-64e0a0e51b2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8. Model Evaluation...\n",
            "Accuracy: 0.9827\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      1.00       121\n",
            "           2       0.97      0.95      0.96        38\n",
            "           3       0.86      0.86      0.86         7\n",
            "           4       1.00      1.00      1.00         7\n",
            "\n",
            "    accuracy                           0.98       173\n",
            "   macro avg       0.96      0.95      0.95       173\n",
            "weighted avg       0.98      0.98      0.98       173\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[121   0   0   0]\n",
            " [  1  36   1   0]\n",
            " [  0   1   6   0]\n",
            " [  0   0   0   7]]\n",
            "Akurasi pada training set:  0.9961414790996784\n",
            "Precision pada training set:  0.9961414790996784\n",
            "Recall pada training set:  0.9961414790996784\n",
            "Akurasi pada test set:  0.9826589595375722\n",
            "Precision pada test set:  0.9826589595375722\n",
            "Recall pada test set:  0.9826589595375722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 9. CREATE MODEL COMPONENTS FOR SAVING\n",
        "# ===============================================================\n",
        "print(\"\\n9. Creating and saving model components...\")\n",
        "\n",
        "# Create a dictionary of components\n",
        "model_components = {\n",
        "    'model': best_model,\n",
        "    'feature_names': X.columns.tolist(),\n",
        "    'encoding_maps': encoding_maps,\n",
        "    'model_params': best_params,\n",
        "    'removed_features': list(corr_features) if len(corr_features) > 0 else [],\n",
        "    'target_map': class_\n",
        "}\n",
        "\n",
        "# Save model components\n",
        "joblib.dump(model_components, 'car_evaluation_prediction_components.joblib')\n",
        "print(\"Model components saved successfully as 'car_evaluation_prediction_components.joblib'\")\n",
        "\n",
        "# Function to predict class (for testing)\n",
        "def predict_class(data, model_components):\n",
        "    \"\"\"\n",
        "    Make class predictions using the trained model\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : dict or DataFrame\n",
        "        Data with features for prediction\n",
        "    model_components : dict\n",
        "        Dictionary containing model and preprocessing information\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    prediction : int\n",
        "        Predicted class as integer (1: 'unacc', 2: 'acc', 3: 'good', 4: 'vgood')\n",
        "    prediction_label : str\n",
        "        Original class label\n",
        "    probability : float\n",
        "        Probability of the predicted class\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # Convert single record to DataFrame if needed\n",
        "    if isinstance(data, dict):\n",
        "        data = pd.DataFrame([data])\n",
        "\n",
        "    # Get components\n",
        "    model = model_components['model']\n",
        "    encoding_maps = model_components['encoding_maps']\n",
        "    feature_names = model_components['feature_names']\n",
        "\n",
        "    # Apply encoding to categorical features\n",
        "    for col in data.columns:\n",
        "        if col in encoding_maps and col != 'class':\n",
        "            data[col] = data[col].map(encoding_maps[col])\n",
        "\n",
        "    # Ensure we only use features that the model was trained on\n",
        "    data_for_pred = data[feature_names].copy()\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(data_for_pred)[0]\n",
        "    probabilities = model.predict_proba(data_for_pred)[0]\n",
        "\n",
        "    # Get inverse mapping for class\n",
        "    class_map_inverse = {v: k for k, v in encoding_maps['class'].items()}\n",
        "    prediction_label = class_map_inverse[prediction]\n",
        "\n",
        "    # Mapping to integer values as specified\n",
        "    class_mapping = {'unacc': 1, 'acc': 2, 'good': 3, 'vgood': 4}\n",
        "    prediction_int = class_mapping[prediction_label]\n",
        "\n",
        "    return {\n",
        "        'prediction': prediction_int,\n",
        "        'prediction_label': prediction_label,\n",
        "        'probability': probabilities[prediction]\n",
        "    }\n",
        "\n",
        "# Test the prediction function with a sample\n",
        "test_sample = X_test.iloc[0].to_dict()\n",
        "loaded_components = joblib.load('car_evaluation_prediction_components.joblib')\n",
        "prediction_result = predict_class(test_sample, loaded_components)\n",
        "\n",
        "print(\"\\nTest prediction result:\")\n",
        "print(prediction_result)\n",
        "print(\"Actual value:\", y_test.iloc[0])"
      ],
      "metadata": {
        "id": "sLatx7srevyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4515c20b-257b-49f7-d4a6-2241027f739f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9. Creating and saving model components...\n",
            "Model components saved successfully as 'car_evaluation_prediction_components.joblib'\n",
            "\n",
            "Test prediction result:\n",
            "{'prediction': 2, 'prediction_label': 'acc', 'probability': np.float64(0.0)}\n",
            "Actual value: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}